{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9951756,"sourceType":"datasetVersion","datasetId":6090025},{"sourceId":9973671,"sourceType":"datasetVersion","datasetId":5894620}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-11-19T10:30:13.164124Z","iopub.execute_input":"2024-11-19T10:30:13.164437Z","iopub.status.idle":"2024-11-19T10:30:23.728350Z","shell.execute_reply.started":"2024-11-19T10:30:13.164410Z","shell.execute_reply":"2024-11-19T10:30:23.727412Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%capture\n# important imports, do not modify this cell\nimport transformers\nimport subprocess as sp\nimport os, gc, sys\nimport torch\nfrom torch import nn, optim\nimport pandas as pd\nimport numpy as np\nimport multiprocessing, time, warnings\nimport matplotlib.pyplot as plt\nimport psutil\n\n# for XLM-RoBERTa model\nfrom transformers import AutoTokenizer, XLMRobertaModel\n# for DistillBERT\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification\n# for ALBERT\nfrom transformers import AlbertTokenizer, AlbertModel\n# for RoBERTa model\nfrom transformers import RobertaTokenizer, RobertaModel\n\n# suppress warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T10:30:23.730165Z","iopub.execute_input":"2024-11-19T10:30:23.730684Z","iopub.status.idle":"2024-11-19T10:30:29.761047Z","shell.execute_reply.started":"2024-11-19T10:30:23.730655Z","shell.execute_reply":"2024-11-19T10:30:29.760091Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"print(\"Choose one of the models to continue:- 1) RoBERTa 2) ALBERT 3) DistillBERT 4) XLM-RoBERTa\")\ntry:\n    val = int(input())\n    assert val in list(range(1,5))\n    if val == 1:\n        model_name = \"RoBERTa Base\"\n        # use RoBERTa-BASE model with input embedding size 768\n        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n        model = RobertaModel.from_pretrained('roberta-base')\n        # change the dense layer at end to be a classification head (3 categories for sentiment analysis)\n        model.pooler.dense = nn.Linear(768, 3)  \n    elif val == 2:\n        model_name = \"ALBERT\"\n        # use ALBERT model with input dimensions 768\n        tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n        model = AlbertModel.from_pretrained(\"albert-base-v2\")\n        # change the dense layer at end to be a classification head (3 categories for sentiment analysis)\n        model.pooler = nn.Linear(768, 3)\n        model.pooler_activation = nn.Tanh()\n    elif val == 3:\n        model_name = \"DistillBERT\"\n        # use DistillBERT model with input dimensions 768\n        tokenizer = AutoTokenizer.from_pretrained(\"Tejas3/distillbert_base_uncased_80\")\n        model = AutoModelForSequenceClassification.from_pretrained(\"Tejas3/distillbert_base_uncased_80\")\n        # change the dense layer at end to be a classification head (3 categories for sentiment analysis)\n        model.vocab_projector = nn.Linear(768, 3)\n    else:\n        model_name = \"XLM-RoBERTa Base\"\n        # use XLM-RoBERTa BASE model with embedding dimensions 768\n        tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n        model = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n        # change the dense layer at end to be a classification head (3 categories for sentiment analysis)\n        model.pooler.dense = nn.Linear(768, 3)\nexcept AssertionError:\n    print(\"Value error, please try again.(run this cell again to continue)\")\nexcept:\n    print(\"Program has run into an exception, please try again.(run this cell again to continue)\")\nfinally:\n    print(\"Selection logic end!\")","metadata":{"execution":{"iopub.status.busy":"2024-11-19T10:30:29.762190Z","iopub.execute_input":"2024-11-19T10:30:29.762995Z","iopub.status.idle":"2024-11-19T10:30:37.475208Z","shell.execute_reply.started":"2024-11-19T10:30:29.762956Z","shell.execute_reply":"2024-11-19T10:30:37.474336Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Choose one of the models to continue:- 1) RoBERTa 2) ALBERT 3) DistillBERT 4) XLM-RoBERTa\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 3\n"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0dc98a30c5d4c9a9852284c6ef544eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"529d58c26fd840678294e82088db51fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"497a8e980d2b4abfa2fafab9230764f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21bba64d89e44386bc5be7495a1ae6fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa8ea339cef149f9bc1f1b6c726fc31c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18337efacebb401ea1d4d8d078ee57fd"}},"metadata":{}},{"name":"stdout","text":"Selection logic end!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# prints number of parameters in the model\nc = 0\nfor p in model.parameters():\n    c += (p.numel())\nprint(\"Number of parameters in model {} (in millions) is {:.4f}\".format(model_name, c/1e6)) \n\n'''\n11.095 million parameters in Alberta \n124.0573 million parameters in RoBERTa Base\n66.9573 million parameters in DistillBERT\n277.4554 million parameters in XLMR-RoBERTa Base\n'''","metadata":{"execution":{"iopub.status.busy":"2024-11-19T10:30:45.531208Z","iopub.execute_input":"2024-11-19T10:30:45.531718Z","iopub.status.idle":"2024-11-19T10:30:45.544055Z","shell.execute_reply.started":"2024-11-19T10:30:45.531664Z","shell.execute_reply":"2024-11-19T10:30:45.542769Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of parameters in model DistillBERT (in millions) is 66.9581\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'\\n11.095 million parameters in Alberta \\n124.0573 million parameters in RoBERTa Base\\n66.9573 million parameters in DistillBERT\\n277.4554 million parameters in XLMR-RoBERTa Base\\n'"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"Dataloader reference:- https://stackoverflow.com/questions/44429199/how-to-load-a-list-of-numpy-arrays-to-pytorch-dataset-loader\n\nBERT model variations reference:- https://360digitmg.com/blog/bert-variants-and-their-differences \n\nReference for Jupyter markdown cell formatting:- IBM Documentation\n\nBERT implementation from scratch and related explanation:- https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891\n\nBERT paper reference:- https://arxiv.org/pdf/1810.04805\n\nLogical processors a system and hyperthreading:- https://unix.stackexchange.com/questions/743285/difference-between-cores-and-logical-processors\n\nGPU P100 vs GPU T4 x2:- https://www.linkedin.com/pulse/kaggle-accelerators-comparison-rukshar-alam-ki9bc","metadata":{}},{"cell_type":"code","source":"batch_size_en ,batch_size_th = 50, 25\n\nstart_time = time.time()\n\nsys.path.insert(1,\"/kaggle/input/minor-project-helper\")\nimport twitter_dataset_preprocess, thai_dataset_preprocess\n\nnlp_dataloaders = {} # dictionary for storing the dataloaders for different language datasets\n\nnlp_dataloaders['twitter'] = twitter_dataset_preprocess.get_data_loaders(batch_size_en)\nnlp_dataloaders['thai'] = thai_dataset_preprocess.get_data_loaders(batch_size_th)\n\nend_time = time.time()\nprint(f\"Created {len(nlp_dataloaders)} language datasets in {end_time-start_time:.10f} seconds.\")\n\n# perform garbage collection and cache clearing\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-19T10:30:47.227602Z","iopub.execute_input":"2024-11-19T10:30:47.228367Z","iopub.status.idle":"2024-11-19T10:30:49.875771Z","shell.execute_reply.started":"2024-11-19T10:30:47.228329Z","shell.execute_reply":"2024-11-19T10:30:49.874951Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Created 2 language datasets in 2.5201516151 seconds.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"sentences, labels = next(iter(nlp_dataloaders['twitter'][0]))\nencoded_input = tokenizer(list(sentences), return_tensors='pt', padding=True)\noutput = model(**encoded_input) # output of the chosen model\n\n# perform garbage collection and cache clearing\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-19T10:30:49.877590Z","iopub.execute_input":"2024-11-19T10:30:49.878241Z","iopub.status.idle":"2024-11-19T10:30:51.513173Z","shell.execute_reply.started":"2024-11-19T10:30:49.878200Z","shell.execute_reply":"2024-11-19T10:30:51.512405Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ROUGH\n# output.pooler_output.shape # for ALBERT, XLMR-RoBERTa Base and RoBERTa Base models\n# output.logits # for DistillBERT model","metadata":{"execution":{"iopub.status.busy":"2024-11-19T10:30:51.514273Z","iopub.execute_input":"2024-11-19T10:30:51.514610Z","iopub.status.idle":"2024-11-19T10:30:51.518374Z","shell.execute_reply.started":"2024-11-19T10:30:51.514566Z","shell.execute_reply":"2024-11-19T10:30:51.517504Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# defining the devices on which training needs to be done\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# defining the optimizer and learning rate scheduler \noptimizer = optim.Adam(model.parameters(), lr = 0.003, weight_decay = 0.0001)\n\n# wait for one epoch while metric has stopped improving, then reduce lr by 30%\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.7, patience = 1) \n\n# defining the evaluation metric to be used, to compare probabilities predicted and actual labels\ncriterion = nn.CrossEntropyLoss()\n\n# perform garbage collection and cache clearing\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-19T10:30:51.520201Z","iopub.execute_input":"2024-11-19T10:30:51.520863Z","iopub.status.idle":"2024-11-19T10:30:52.158934Z","shell.execute_reply.started":"2024-11-19T10:30:51.520826Z","shell.execute_reply":"2024-11-19T10:30:52.158221Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# load model from checkpoint\n\nif model_name == \"XLM-RoBERTa Base\":\n    checkpoint_file_name = \"XLMR_RoBERTa_Base_checkpoint.pth\"\nelif model_name == \"RoBERTa Base\":\n    checkpoint_file_name = \"RoBERTa_Base_checkpoint.pth\"\nelse:\n    checkpoint_file_name = model_name + \"_checkpoint.pth\"\n\ncheckpoint_directory = \"/kaggle/input/minorproject-checkpoints/\"\ncheckpoint_dict = torch.load(checkpoint_directory + checkpoint_file_name, map_location = device)\n\ntry:\n    model.load_state_dict(checkpoint_dict['model_state_dict'])\n    print(\"Model has been loaded from the checkpoint successfully!\")\nexcept:\n    print(\"An exception has occured. Please try again!\")\nfinally:\n    # perform garbage collection and cache clearing\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T11:29:20.664476Z","iopub.execute_input":"2024-11-18T11:29:20.665308Z","iopub.status.idle":"2024-11-18T11:29:31.355301Z","shell.execute_reply.started":"2024-11-18T11:29:20.665265Z","shell.execute_reply":"2024-11-18T11:29:31.354354Z"}},"outputs":[{"name":"stdout","text":"Model has been loaded from the checkpoint successfully!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# for logging GPU memory usage in Python\ndef get_gpu_memory():\n    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n    print(\"Available GPU:\", memory_free_values, \"MB\")\n    \n# for logging RAM memory usage in Python\ndef get_ram_memory():\n    # Get information about the system's virtual memory\n    vm = psutil.virtual_memory()\n\n    # Get the available memory\n    ram_available = vm.available / 1024**2\n\n    print(\"Available RAM:\", ram_available, \"MB\")\n\n# perform garbage collection and cache clearing\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-19T10:30:55.014473Z","iopub.execute_input":"2024-11-19T10:30:55.014950Z","iopub.status.idle":"2024-11-19T10:30:55.151125Z","shell.execute_reply.started":"2024-11-19T10:30:55.014922Z","shell.execute_reply":"2024-11-19T10:30:55.150170Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## TODOs\n\n1) Find out why importing torchtext library gives error, while no such error for torchvision or torch\n\n2) Find out why passing Thai sentences to RoBERTa model, in torchtext, didn't give large embedding error, as obtained in case of ALBERT used here \n\n3) Find out why importing PyTorch/XLA gives error","metadata":{}},{"cell_type":"code","source":"nlp_parallel_model = nn.DataParallel(model)\nnlp_parallel_model.to(device)\n\n# check for cuda availability\ncuda_is_available = torch.cuda.is_available()\n\n# define sample size for which logs to be taken\nsample_size_th = 5000\n\n# define training loop within a modularised funtion\n# is_log_needed: Boolean variable defining the need for computing logs or not. If set to False, no logs are recorded and training done on entire dataset\ndef train(train_dataloader, validation_dataloader, optimizer, epochs, is_log_needed):\n    training_loss_list, validation_loss_list = [], []\n    for e in range(1, epochs+1): \n        training_loss = 0.0\n        validation_loss = 0.0\n        time1 = time.time() # start the timer\n        \n        # define the number of samples processed across each epoch\n        processed_samples = 0  \n        \n        # put the model in training mode\n        nlp_parallel_model.train()\n        for sentence, label in train_dataloader:\n            length_samples = len(sentence)\n            \n            # zero out the gradients in the optimizer, to avoid accumulation\n            optimizer.zero_grad()\n            \n            if processed_samples == sample_size_th and is_log_needed:\n                print(\"After clearing gradients from the PyTorch optimizer, free memory is logged as....\")\n                if cuda_is_available:\n                    get_gpu_memory()\n                get_ram_memory()\n            \n            encoded_input = tokenizer(list(sentence), padding=True, return_tensors='pt', truncation=True, max_length=512, add_special_tokens = True)\n            del sentence\n            \n            if processed_samples == sample_size_th and is_log_needed:\n                print(\"After tokenizing the sentences, free memory is logged as....\")\n                if cuda_is_available:\n                    get_gpu_memory()\n                get_ram_memory()\n                \n            output = nlp_parallel_model(**encoded_input)\n            del encoded_input\n            \n            if processed_samples == sample_size_th and is_log_needed:\n                print(\"After processing the tokenized sentences, free memory is logged as....\")\n                if cuda_is_available:\n                    get_gpu_memory()\n                get_ram_memory()\n            \n            label = torch.Tensor(label).to(device, dtype=torch.int64)\n            if model_name == \"DistillBERT\":\n                output = output.logits.to(device)\n            else:\n                output = output.pooler_output.to(device)\n                \n            if processed_samples == sample_size_th and is_log_needed:\n                print(\"After computing the tensor for output values, free memory is logged as....\")\n                if cuda_is_available:\n                    get_gpu_memory()\n                get_ram_memory()\n            \n            loss = criterion(output, label)\n            \n            if processed_samples == sample_size_th and is_log_needed:\n                print(\"After computing the loss, free memory is logged as....\")\n                if cuda_is_available:\n                    get_gpu_memory()\n                get_ram_memory()\n            \n            del output, label\n\n            loss.backward(retain_graph = False)\n            optimizer.step()\n            \n            if processed_samples == sample_size_th and is_log_needed:\n                print(\"After computing loss gradients and performing backpropagation, free memory is logged as....\")\n                if cuda_is_available:\n                    get_gpu_memory()\n                get_ram_memory()\n            \n            training_loss += loss.item()\n            \n            if processed_samples == sample_size_th and is_log_needed:\n                # break when 5000 samples are processed\n                time2 = time.time() # stop the timer\n                print(\"Time taken to train per 5000 samples-> {:.4f} seconds\".format(time2-time1))\n                return \"Usage log tests being done!\"\n                \n            processed_samples += length_samples \n            del length_samples\n        \n            # try deallocating heavy objects to free up memory and GPU required for training\n            # needed as Python's Garbage collector does not work automatically as in Java.\n            del loss\n            gc.collect()\n            torch.cuda.empty_cache() \n        \n        # put model in validation mode\n        nlp_parallel_model.eval()\n        with torch.no_grad():\n            for sentence, label in validation_dataloader:\n                encoded_input = tokenizer(list(sentence), padding=True, return_tensors='pt', truncation=True, max_length=512, add_special_tokens = True)\n                output = nlp_parallel_model(**encoded_input)\n                del sentence\n\n                label = torch.Tensor(label).to(device, dtype=torch.int64)\n                if model_name == \"DistillBERT\":\n                    output = output.logits.to(device)\n                else:\n                    output = output.pooler_output.to(device)\n                loss = criterion(output, label)\n                del output, label\n\n                validation_loss += loss.item()\n\n                # try deallocating heavy objects to free up memory and GPU required for training\n                # needed as Python's Garbage collector does not work automatically as in Java.\n                del loss\n                gc.collect()\n                torch.cuda.empty_cache() \n        \n        time2 = time.time() # stop the timer \n        lr_scheduler.step(validation_loss)\n        print(\"Epoch {}-> Training loss {:.10f}, Validation loss {:.10f}, Time taken {:.10f} seconds\".format(e, training_loss / len(train_dataloader), validation_loss / len(validation_dataloader), time2-time1))\n        training_loss_list.append(training_loss / len(train_dataloader))\n        validation_loss_list.append(validation_loss / len(validation_dataloader))\n    return (training_loss_list, validation_loss_list)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T10:30:56.099980Z","iopub.execute_input":"2024-11-19T10:30:56.100396Z","iopub.status.idle":"2024-11-19T10:30:56.368899Z","shell.execute_reply.started":"2024-11-19T10:30:56.100366Z","shell.execute_reply":"2024-11-19T10:30:56.367941Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"**<p style=\"text-align:center;\"> RAM / GPU Usage and Training Time LOG**\n    \n__<u>Note that the readings are taken for processing per 5000 training samples for both English and Thai datasets. Batch sizes for English and Thai datasets is assumed to be 50 and 25 respectively, unless stated otherwise. The tests for English and Thai datasets have been done together orderwise to determine which language requires more computational resources during sentence processing.</u>__\n    \n__<u>It shall be assumed that observations are recorded for English followed by Thai.</u>__\n    \n__<u>It shall be assumed that Adam optimizer is being used, unless stated otherwise.</u>__\n    \n<br> __1==>__ _Using ALBERT model:-_ \n   \n* Using CPU: \n    \n    _Getting sentence embedding mismatch error for thai dataset, as ALBERT can process only 512 tokens (context length),     <br> while embedding sizes greter than 512 tokens is also being obtained._\n    <br> __This occurs due to model assigning more tokens to words outside of its vocabulary set.__\n    <br> _Reference for error:-_ https://stackoverflow.com/questions/64320883/the-size-of-tensor-a-707-must-match-the-size-of-tensor-b-512-at-non-singleto \n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available RAM: 23849.35546875 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available RAM: 23849.35546875 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available RAM: 24313.61328125 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available RAM: 24313.61328125 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available RAM: 24313.61328125 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available RAM: 24260.5 MB\n    <br> Time taken to train per 5000 samples-> **1319.1426 seconds (~22 minutes)**\n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available RAM: 24454.15625 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available RAM: 24454.15625 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available RAM: 24454.15625 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available RAM: 24454.15625 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available RAM: 24454.15625 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available RAM: 24454.15625 MB\n    <br> Time taken to train per 5000 samples-> **3227.5199 seconds (~54 minutes)** \n\n<br> __2==>__ _Using RoBERTa model:-_ \n   \n* Using CPU: \n    * English Dataset:- <br>\n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available RAM: 23727.98046875 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available RAM: 23727.98046875 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available RAM: 23726.83984375 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available RAM: 23726.83984375 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available RAM: 23726.83984375 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available RAM: 23733.60546875 MB\n    <br> Time taken to train per 5000 samples-> **1155.1060 seconds (~20 minutes)**\n    \n    * Thai Dataset:- <br>\n    <br> **Taking more than 3 hours to process 5000 samples with RAM usage upto 24-26 GB**\n    \n    It can be hence seen that RoBERTa occupies huge memory and takes more processing time, hence I feel     <br> XLM-RoBERTa should not have its logs recorded, as it is based on RoBERTa architecture. For         <br> DistillBERT, I feel CPU is inefficient, so enough logs have been taken.   \n    \n__Conclusion-__ _As it can be seen, CPU is inefficient at training tasks, as it's instruction cycle is optimised for small dimensions of data. For higher dimensional data, GPUs are preferred as they promote multiprocessing by allocating a thread to each GPU core, with thousands of core present inside a GPU device. For small scale data, CPU outperforms GPU due to optimised fetch-decode-execute cycle     of CPU and additional overhead of creating threads for subunits of data in case of GPU._\n\n_Also, it can be seen that Thai sentences require large amount of training time, which seems       obvious as ALBERT model is not trained on Thai and the words used in Thai are outside the               vocabulary set of Thai._\n    \n___Thus, we will be using GPU for recording logs. Two types of GPUs will be used:- GPU P100 and GPU T4 x2___ \n    \n<u><em>Training on GPU P100 and GPU T4 x2:-</em></u>\n    \n__1==>__ _Using ALBERT model:-_ \n    \n* Using GPU P100:\n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available GPU: [15668] MB\n    <br> Available RAM: 26858.90625 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available GPU: [15668] MB\n    <br> Available RAM: 26858.875 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available GPU: [13106] MB\n    <br> Available RAM: 26848.046875 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available GPU: [13106] MB\n    <br> Available RAM: 26848.046875 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available GPU: [13106] MB\n    <br> Available RAM: 26848.046875 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available GPU: [12996] MB\n    <br> Available RAM: 26848.046875 MB\n    <br> Time taken to train per 5000 samples-> **52.5883 seconds**\n\n    For Thai dataset, **OutOfMemoryError** came as the GPU usage exceeded permissible limit. (16 GiB)\n    \n* Using GPU T4 x2:\n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available GPU: [14631, 14835] MB\n    <br> Available RAM: 26296.4609375 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available GPU: [14631, 14835] MB\n    <br> Available RAM: 26295.2109375 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available GPU: [13087, 13193] MB\n    <br> Available RAM: 26296.84765625 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available GPU: [13087, 13193] MB\n    <br> Available RAM: 26296.07421875 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available GPU: [13087, 13193] MB\n    <br> Available RAM: 26296.07421875 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available GPU: [13027, 13133] MB\n    <br> Available RAM: 26287.22265625 MB\n    <br> Time taken to train per 5000 samples-> **69.0760 seconds (~1 minute)**\n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available GPU: [14627, 14835] MB\n    <br> Available RAM: 26249.203125 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available GPU: [14627, 14835] MB\n    <br> Available RAM: 26245.515625 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available GPU: [14003, 14153] MB\n    <br> Available RAM: 26245.125 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available GPU: [14003, 14153] MB\n    <br> Available RAM: 26244.828125 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available GPU: [14003, 14153] MB\n    <br> Available RAM: 26244.828125 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available GPU: [13983, 14133] MB\n    <br> Available RAM: 26244.48046875 MB\n    <br> Time taken to train per 5000 samples-> **115.7149 seconds (~2 minutes)**\n   \n__2==>__ _Using RoBERTa model:-_ \n    \n* Using GPU P100:    \n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available GPU: [13690] MB\n    <br> Available RAM: 28158.8046875 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available GPU: [13690] MB\n    <br> Available RAM: 28169.6953125 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available GPU: [12346] MB\n    <br> Available RAM: 28159.0 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available GPU: [12346] MB\n    <br> Available RAM: 28158.47265625 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available GPU: [12346] MB\n    <br> Available RAM: 28177.46484375 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available GPU: [12090] MB\n    <br> Available RAM: 28166.8828125 MB\n    <br> Time taken to train per 5000 samples-> **45.3848 seconds**\n    \n    For Thai dataset, **OutOfMemoryError** came as the GPU usage exceeded permissible limit. (16 GiB)\n    \n* Using GPU T4 x2:\n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available GPU: [12489, 14835] MB\n    <br> Available RAM: 27663.0078125 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available GPU: [12489, 14835] MB\n    <br> Available RAM: 27662.9921875 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available GPU: [12455, 13605] MB\n    <br> Available RAM: 27662.4140625 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available GPU: [12455, 13605] MB\n    <br> Available RAM: 27662.1796875 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available GPU: [12455, 13605] MB\n    <br> Available RAM: 27662.1796875 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available GPU: [12207, 13457] MB\n    <br> Available RAM: 27659.9140625 MB\n    <br> Time taken to train per 5000 samples-> **78.9255 seconds (~1 minute 30 seconds)**\n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available GPU: [12659, 14833] MB\n    <br> Available RAM: 27617.72265625 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available GPU: [12659, 14833] MB\n    <br> Available RAM: 27617.48828125 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available GPU: [5037, 6841] MB\n    <br> Available RAM: 27616.4140625 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available GPU: [5037, 6841] MB\n    <br> Available RAM: 27616.4140625 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available GPU: [5037, 6841] MB\n    <br> Available RAM: 27615.99609375 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available GPU: [4803, 6841] MB\n    <br> Available RAM: 27615.984375 MB\n    <br> Time taken to train per 5000 samples-> **354.7913 seconds (~6 minutes)**\n    \n__3==>__ _Using DistillBERT model:-_ \n    \n* Using GPU P100:    \n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available GPU: [14564] MB\n    <br> Available RAM: 28590.79296875 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available GPU: [14564] MB\n    <br> Available RAM: 28590.078125 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available GPU: [14378] MB\n    <br> Available RAM: 28590.078125 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available GPU: [14378] MB\n    <br> Available RAM: 28590.078125 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available GPU: [14378] MB\n    <br> Available RAM: 28590.078125 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available GPU: [14268] MB\n    <br> Available RAM: 28590.11328125 MB\n    <br> Time taken to train per 5000 samples-> **31.4232 seconds**\n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available GPU: [14654] MB\n    <br> Available RAM: 28595.55859375 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available GPU: [14654] MB\n    <br> Available RAM: 28595.5390625 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available GPU: [13750] MB\n    <br> Available RAM: 28595.37109375 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available GPU: [13750] MB\n    <br> Available RAM: 28595.35546875 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available GPU: [13750] MB\n    <br> Available RAM: 28595.35546875 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available GPU: [13474] MB\n    <br> Available RAM: 28595.21875 MB\n    <br> Time taken to train per 5000 samples-> **60.8670 seconds (~1 minute)**\n    \n* Using GPU T4 x2:\n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available GPU: [13513, 14835] MB\n    <br> Available RAM: 28342.21484375 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available GPU: [13513, 14835] MB\n    <br> Available RAM: 28343.65234375 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available GPU: [13503, 14211] MB\n    <br> Available RAM: 28350.3671875 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available GPU: [13503, 14211] MB\n    <br> Available RAM: 28349.70703125 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available GPU: [13503, 14211] MB\n    <br> Available RAM: 28336.08984375 MB\n    <br> Available GPU: [13393, 14121] MB\n    <br> Available RAM: 28335.90234375 MB\n    <br> Time taken to train per 5000 samples-> **55.7659 seconds**\n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available GPU: [13599, 14833] MB\n    <br> Available RAM: 28306.3203125 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available GPU: [13599, 14833] MB\n    <br> Available RAM: 28294.51953125 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available GPU: [13589, 14341] MB\n    <br> Available RAM: 28294.62890625 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available GPU: [13589, 14341] MB\n    <br> Available RAM: 28294.3828125 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available GPU: [13589, 14341] MB\n    <br> Available RAM: 28294.3828125 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available GPU: [13399, 14251] MB\n    <br> Available RAM: 28298.25 MB\n    <br> Time taken to train per 5000 samples-> **89.3750 seconds (~1 minute 30 seconds)** \n    \n__Conclusion:-__ _It can be clearly observed that ALBERT and RoBERTa fail to train even 5000 samples for Thai dataset on GPU P100, but able to do so easily on GPU T4 x2. Even though GPU P100 is ideal for training, one plausible reason that can be stated is that it provides less memory compared to T4 x2. Two things can also be concluded henceforth:-\n<br> 1) ALBERT and RoBERTa are memory intensive models, especially when working with non-English languages.\n<br> 2) RoBERTa consumes much memory on T4 x2. Thus, for multilingual version XLMR, we will assume T4 x2 works best.\n<br> One more thing that can be seen is DistillBERT achieves low latency, in terms of time and memory , be it GPU P100 or GPU T4 x2. This shows that using distilled network, taking in semantic knowledge from larger teacher network (BERT) plays a major role in computation time reduction.\n<br> Interesting thing is that ALBERT is designed to reduce model parameters. Yet, it is memory intensive._\n    \n__Assuming that GPU T4 x2 is ideal for training the models, result log for XLM-RoBERTa BASE is shown below:__\n    \n* Using GPU T4 x2:\n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available GPU: [9861, 14835] MB\n    <br> Available RAM: 27270.69140625 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available GPU: [9861, 14835] MB\n    <br> Available RAM: 27270.31640625 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available GPU: [9821, 12873] MB\n    <br> Available RAM: 27269.25390625 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available GPU: [9821, 12873] MB\n    <br> Available RAM: 27268.0625 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available GPU: [9821, 12873] MB\n    <br> Available RAM: 27264.57421875 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available GPU: [9087, 12139] MB\n    <br> Available RAM: 27264.5234375 MB\n    <br> Time taken to train per 5000 samples-> **92.7214 seconds (~1 minute 30 seconds)**\n    \n    After clearing gradients from the PyTorch optimizer, free memory is logged as....\n    <br> Available GPU: [9341, 14833] MB\n    <br> Available RAM: 27237.44140625 MB\n    <br> After tokenizing the sentences, free memory is logged as....\n    <br> Available GPU: [9341, 14833] MB\n    <br> Available RAM: 27237.20703125 MB\n    <br> After processing the tokenized sentences, free memory is logged as....\n    <br> Available GPU: [9313, 12915] MB\n    <br> Available RAM: 27225.46875 MB\n    <br> After computing the tensor for output values, free memory is logged as....\n    <br> Available GPU: [9313, 12915] MB\n    <br> Available RAM: 27225.375 MB\n    <br> After computing the loss, free memory is logged as....\n    <br> Available GPU: [9313, 12915] MB\n    <br> Available RAM: 27225.56640625 MB\n    <br> After computing loss gradients and performing backpropagation, free memory is logged as....\n    <br> Available GPU: [9233, 12915] MB\n    <br> Available RAM: 27224.55859375 MB\n    <br> Time taken to train per 5000 samples-> **201.5807 seconds (~3 minutes 30 seconds)**\n    \n__Conclusion:-__ _It can be seen that for GPU T4 x2, RoBERTa seems to perform worse that XLM-RoBERTa (expected since XLMR can handle multiple languages), but for English, performance of XLMR is worst._\n\nOne common thing that is missed during model testing is that all the GPU cores are not utilised, even when it is done properly during training. This may cause issues like CudaError when testing on bulk language datasets. To counter this, we are using DataParallel wrapper to distribute model processing across all GPU cores, irrespective of whether it is GPU P100/GPU T4 x2.\n    ","metadata":{}},{"cell_type":"code","source":"msg = \"\"\n\nif cuda_is_available:\n    msg = \"Voila! You have set it up all correct.\"\n    num_gpu_cores = torch.cuda.device_count()\n    if num_gpu_cores < 2:  # just to be on safe side, using T4 x2 for more memory\n        msg = \"Oops! Wrong training hardware chosen!\"\n\nprint(msg)\nassert msg == \"Voila! You have set it up all correct.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T10:30:57.326078Z","iopub.execute_input":"2024-11-19T10:30:57.326755Z","iopub.status.idle":"2024-11-19T10:30:57.331583Z","shell.execute_reply.started":"2024-11-19T10:30:57.326720Z","shell.execute_reply":"2024-11-19T10:30:57.330668Z"}},"outputs":[{"name":"stdout","text":"Voila! You have set it up all correct.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# call training modules for English and Thai language datasets\ntry:\n    if cuda_is_available:\n        # change hyperparameters here\n        epochs = 3\n        optimizer.param_groups[0]['lr'] = 0.03\n        optimizer.param_groups[0]['weight_decay'] = 0.007\n        \n        is_log_needed = bool(int(input(\"Want to record logs or not(0/1)?:->\"))) # we require logs to be computed, thus setting is_log_needed to be True\n        if not is_log_needed:\n            print(\"Training {} model with learning rate: {}, weight decay: {} for {} number of epochs\".format(model_name, optimizer.param_groups[0]['lr'], optimizer.param_groups[0]['weight_decay'], epochs))\n        l = train(nlp_dataloaders['twitter'][0], nlp_dataloaders['twitter'][1], optimizer, epochs, is_log_needed)\n        l = train(nlp_dataloaders['thai'][0], nlp_dataloaders['thai'][1], optimizer, epochs, is_log_needed)\n    else:\n        print(\"Please train on GPUs!\")\n        raise AssertionError(\"Appropriate training hardware unit not chosen\")\nexcept:\n    print(\"Exception has occured! Please try again.\")","metadata":{"execution":{"iopub.status.busy":"2024-11-19T10:30:59.930021Z","iopub.execute_input":"2024-11-19T10:30:59.930701Z","iopub.status.idle":"2024-11-19T12:14:39.055356Z","shell.execute_reply.started":"2024-11-19T10:30:59.930667Z","shell.execute_reply":"2024-11-19T12:14:39.054400Z"},"trusted":true},"outputs":[{"output_type":"stream","name":"stdin","text":"Want to record logs or not(0/1)?:-> 0\n"},{"name":"stdout","text":"Training DistillBERT model with learning rate: 0.03, weight decay: 0.007 for 3 number of epochs\nEpoch 1-> Training loss 0.9807632326, Validation loss 0.6863198912, Time taken 1670.5965814590 seconds\nEpoch 2-> Training loss 1.8362497858, Validation loss 0.6860823764, Time taken 1664.6386559010 seconds\nEpoch 3-> Training loss 0.6924033436, Validation loss 0.7234542374, Time taken 1652.1677691936 seconds\nEpoch 1-> Training loss 4.5085909089, Validation loss 0.9921664245, Time taken 407.0619394779 seconds\nEpoch 2-> Training loss 1.2584961391, Validation loss 0.9845785437, Time taken 412.9943861961 seconds\nEpoch 3-> Training loss 0.9818475802, Validation loss 0.9948842360, Time taken 409.3536829948 seconds\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"__<center>Model Training logs</center>__\n\n1) Using XLM-RoBERTa Base model:-\n   <br> Training XLM-RoBERTa Base model with learning rate: 0.003, weight decay: 0.0001 for 1 number of epochs\n   <br> Epoch 1-> Training loss **0.7590388735**, Validation loss **0.7540896507**, Time taken **3059.0003983974 seconds (~51 minutes)**\n   <br> Epoch 1-> Training loss **1.0206603203**, Validation loss **0.9978039391**, Time taken **894.9079222679 seconds (~15 minutes)**\n   <br> Testing on English Language dataset with batch size 50:-\n   <br> Correct predictions **0 out of 30000 test samples**\n   <br> Testing on Thai Language dataset with batch size 25:- \n   <br> Correct predictions **1510 out of 2675 test samples**\n\n    <br> Training XLM-RoBERTa Base model with learning rate: 0.007, weight decay: 0.001 for 2 number of epochs\n    <br> Epoch 1-> Training loss **0.7582048333**, Validation loss **0.7552044338**, Time taken **2969.9936687946 seconds**\n    <br> Epoch 2-> Training loss **0.7570085176**, Validation loss **0.7557407733**, Time taken **2940.7576088905 seconds**\n    <br> Epoch 1-> Training loss **1.0431717821**, Validation loss **0.9942249988**, Time taken **887.8887867928 seconds**\n    <br> Epoch 2-> Training loss **0.9859426652**, Validation loss **0.9968404985**, Time taken **888.1932497025 seconds**\n    <br> Testing on English Language dataset with batch size 50\n    <br> Correct predictions **0 out of 30000 test samples**\n    <br> Testing on Thai Language dataset with batch size 25\n    <br> Correct predictions **1510 out of 2675 test samples**\n\n2) Using DistillBERT model:-\n    <br> Training DistillBERT model with learning rate: 0.03, weight decay: 0.007 for 3 number of epochs\n    <br> Epoch 1-> Training loss **0.9807632326**, Validation loss **0.68631989122**,Time taken **1670.5965814590 seconds (~28 minutes)**\n    <br> Epoch 2-> Training loss **1.8362497858**, Validation loss **0.6860823764**, Time taken **1664.6386559010 seconds**\n    <br> Epoch 3-> Training loss **0.6924033436**, Validation loss **0.7234542374**, Time taken **1652.1677691936 seconds**\n    <br> Epoch 1-> Training loss **4.5085909089**, Validation loss **0.9921664245**, Time taken **407.0619394779 seconds (~7 minutes)**\n    <br> Epoch 2-> Training loss **1.2584961391**, Validation loss **0.9845785437**, Time taken **412.9943861961 seconds**\n    <br> Epoch 3-> Training loss **0.9818475802**, Validation loss **0.9948842360**, Time taken **409.3536829948 seconds**\n    <br> Testing on English Language dataset with batch size 50\n    <br> Correct predictions **0 out of 30000 test samples**\n    <br> Testing on Thai Language dataset with batch size 25\n    <br> Correct predictions **1510 out of 2675 test samples**","metadata":{}},{"cell_type":"code","source":"# Save the trained model parameters in a checkpoint file and save it for future inference purposes\n\nmodel_state_dict = model.state_dict()\noptimizer_name = \"Adam\"\nlr_scheduler_name = \"ReduceLROnPlateau\"\n\ncheckpoint_dict = {\n    'model_state_dict': model_state_dict,\n    'optimizer_name': optimizer_name,\n    'lr': optimizer.param_groups[0]['lr'],\n    'weight_decay': optimizer.param_groups[0]['weight_decay'],\n    'lr_scheduler_name': lr_scheduler_name\n                  }\n\n# Kaggle input files are read-only and cannot be overwritten\n# thus need to download the checkpoint and manually upload it to the dataset\n\nif cuda_is_available:\n    torch.save(checkpoint_dict, '/kaggle/working/' + checkpoint_file_name)\nelse:\n    print(\"Not able to save file as GPU has not been set up!\")\n\n# perform garbage collection and cache clearing\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:14:48.667181Z","iopub.execute_input":"2024-11-19T12:14:48.667742Z","iopub.status.idle":"2024-11-19T12:14:49.347535Z","shell.execute_reply.started":"2024-11-19T12:14:48.667712Z","shell.execute_reply":"2024-11-19T12:14:49.346554Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# test the model's performance\nnlp_parallel_model = nn.DataParallel(model)\nnlp_parallel_model.to(device) # move model to CPU/GPU\n\ndef test(test_dataloader, batch_size):\n    correct_predictions = 0\n    for sentence, label in test_dataloader:\n        encoded_input = tokenizer(list(sentence), padding=True, return_tensors='pt', truncation=True, max_length=512, add_special_tokens = True)\n        del sentence\n        \n        output = nlp_parallel_model(**encoded_input) \n        del encoded_input\n        \n        label = torch.Tensor(label).to(device, dtype=torch.int64)\n        \n        if model_name == \"DistillBERT\":\n            output = output.logits.to(device)\n        else:\n            output = output.pooler_output.to(device)\n            \n        _, output_label = output.topk(k = 1, dim = 1)\n        \n        del output\n\n        output_label = torch.reshape(output_label, (torch.numel(output_label),))\n\n        # correct prediction made by model\n        correct_predictions += int(sum(output_label==label))\n\n        # try deallocating heavy objects to free up memory and GPU required for training\n        # needed as Python's Garbage collector does not work automatically as in Java.\n        del output_label, label\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    # printing the result (observes that model performance has improved slightly with training)\n    print(\"Correct predictions {} out of {} test samples\".format(correct_predictions, len(test_dataloader)*batch_size))\n\nif cuda_is_available:\n    print(\"Testing on English Language dataset with batch size {}\".format(batch_size_en))\n    test(nlp_dataloaders['twitter'][2], batch_size_en)\n    print(\"Testing on Thai Language dataset with batch size {}\".format(batch_size_th))\n    test(nlp_dataloaders['thai'][2], batch_size_th)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:16:37.167640Z","iopub.execute_input":"2024-11-19T12:16:37.168342Z","iopub.status.idle":"2024-11-19T12:20:35.421691Z","shell.execute_reply.started":"2024-11-19T12:16:37.168308Z","shell.execute_reply":"2024-11-19T12:20:35.420840Z"}},"outputs":[{"name":"stdout","text":"Testing on English Language dataset with batch size 50\nCorrect predictions 0 out of 30000 test samples\nTesting on Thai Language dataset with batch size 25\nCorrect predictions 1510 out of 2675 test samples\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TODOs:\n\n1) State proper reasons for GPU P100 being better for training, validation than GPU T4 x2\n\n2) State proper reasons for why ALBERT may be memory intensive\n\n3) State proper reasons for 0% accuracy on English dataset\n\n   Cue:- Due to limitation of available public dataset/ label imbalance, work could not be completed and future work can continue on symmetric datasets with 3 sentiment labels (positive, negative and neutral)","metadata":{}},{"cell_type":"code","source":"'''\nPlausible reason for poor performance on English dataset is that model seems to get stuck in local \nminima by optimising parameters while classifying sentences as irrelevant, while no possible labels are \nthere in original dataset, thereby indicating that class imbalance issue might be there.   \n\n\ncan refer to https://huggingface.co/datasets/prithivMLmods/GPT-Sentiment-Analysis-200K for eng dataset\n'''\n\nsentence, label = next(iter(nlp_dataloaders['twitter'][2]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T15:33:23.307286Z","iopub.execute_input":"2024-11-18T15:33:23.307761Z","iopub.status.idle":"2024-11-18T15:33:23.327277Z","shell.execute_reply.started":"2024-11-18T15:33:23.307716Z","shell.execute_reply":"2024-11-18T15:33:23.325990Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"encoded_input = tokenizer(list(sentence), padding=True, return_tensors='pt', truncation=True, max_length=512, add_special_tokens = True)\noutput = model(**encoded_input)\noutput = output.pooler_output\n\n_, output_label = output.topk(1, dim = -1)\n\noutput_label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T15:35:48.187835Z","iopub.execute_input":"2024-11-18T15:35:48.188304Z","iopub.status.idle":"2024-11-18T15:35:53.945295Z","shell.execute_reply.started":"2024-11-18T15:35:48.188260Z","shell.execute_reply":"2024-11-18T15:35:53.944032Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tensor([[1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1]])"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"label = torch.Tensor(label).to(dtype=torch.int64)\nlabel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T15:37:33.786467Z","iopub.execute_input":"2024-11-18T15:37:33.787221Z","iopub.status.idle":"2024-11-18T15:37:33.796725Z","shell.execute_reply.started":"2024-11-18T15:37:33.787175Z","shell.execute_reply":"2024-11-18T15:37:33.795516Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n        2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 2, 2,\n        2, 0])"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}